{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfqu9icj5jrp"
      },
      "source": [
        "1. What is a parameter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIOW0qd5juK"
      },
      "source": [
        "-> Parameter is value which is describe the characteristic value for population data.\n",
        "\n",
        "For Example:\n",
        "\n",
        "Mean of population, Std Dev of populaton, Variance of population, etc all are defines the value for population data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YV4wcml5jwz"
      },
      "source": [
        "2. What is correlation? What does negative correlation mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6TrZb905jzX"
      },
      "source": [
        "-> Correlation is a value which defines how two features are related to each other\n",
        "\n",
        "It gives the direction and strength of the relation between them\n",
        "\n",
        "for example:\n",
        "\n",
        "if the correlation value is -1 so there is strongly negative correlation between both.\n",
        "\n",
        "if the correlation value is +0.98 so there is strongly positive correlation between both.\n",
        "\n",
        "the sign is describe the direction of it and value is define the strength of it.\n",
        "\n",
        "-> Negative correlation means the relation between two features are negative direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN9peXik5j2W"
      },
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlqAydU_5j43"
      },
      "source": [
        "-> Machine learning is a field in which we try to train a model or an algorithm learns pattern from data and make a prediction on unseen data without explicit coding.\n",
        "\n",
        "Components:\n",
        "\n",
        "\n",
        "Data : Raw data for learning\n",
        "\n",
        "Algorithm : different type of set of instructions for trainning the model and learns the patterns from the data\n",
        "\n",
        "Feature : A relevent columns of the data\n",
        "\n",
        "Model : An algorithm that learns the patterns\n",
        "\n",
        "Training : It is the process of learns the pattern of data by model\n",
        "\n",
        "Testing : It is the process of test the trained model on unseen data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZsCN8BQ5j7Z"
      },
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n12moD_W5j-E"
      },
      "source": [
        "-> loss value defines the error. It defines how far the predicted value is different from the actual value\n",
        "\n",
        "if the loss value is low it defines the model's accuracy is high otherwise loss value is high means model's accuraccy is low"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2T85Rw55kAy"
      },
      "source": [
        "5. What are continuous and categorical variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxa71ffu5kEg"
      },
      "source": [
        "-> Continuos varibale contains the numerical values like heigh, weight, etc.\n",
        "\n",
        "categorical variable contains the text data like male or female, yes or no, high or low, etc. this is called as categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ufD7vCAVUF"
      },
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0dvBZ3AVWg"
      },
      "source": [
        "-> Categorical value contains the characters so we can't trained this on model that's why we use encoding techniques that convert this categorical value into the numerical value\n",
        "\n",
        "there are common techniques for encofing....\n",
        "1. Nomial/ One Hot Encoding\n",
        "2. Ordinal and Label Encoding\n",
        "3. Target Guided ordinal Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5unKQ8lAVY7"
      },
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQorItIQAVby"
      },
      "source": [
        "-> Training dataset is a part of whole dataset using this we train the algorithm (model) to learn patterns and predict the future values from unseen data very efficiently\n",
        "\n",
        "Testing Data : It is remaining part of the whole data using this we test the model and check how the model works on unseen data and check the accuracy of it to make better understadning for build efficient and high performace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHm-Bw6AVeK"
      },
      "source": [
        "8. What is sklearn.preprocessing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuiCOpNWAVgp"
      },
      "source": [
        "-> preprocessing is a module of sciket learn package. it is used for processing the data such that data scaling using minmaxscaler, standardscaler, normlize (unit scaling) and encoding using onehotencoder, labelencoding, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_IO6QzRAVjb"
      },
      "source": [
        "9. What is a Test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHJNxXRGAVl9"
      },
      "source": [
        "-> Test set is a testing data ratio for validate or test the trained model\n",
        "\n",
        "generally we use 70-30 or 80-20 or 90-10 ration in which the majority part is training data and remiaing part will be test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4JII5hlAVoo"
      },
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ON1bwrvAVra"
      },
      "source": [
        "-> for industry level we mainly spilit the train data and test data in this ration 70-30 or 80-20 or 90-10 accrodingly.\n",
        "\n",
        "but commonly we use 70-30 ratio because it's good approach\n",
        "\n",
        "-> a machine problem solving flow:\n",
        "\n",
        "1. define the exact problem\n",
        "2. reaserach on it and find the right and good approach for that problem\n",
        "3. collect the relevent data for it\n",
        "4. convert the raw data into sturctured data using preprocessing, handling missing values, outliers, scaling, encoding, etc\n",
        "5. split the data into train and test data\n",
        "6. train the approprite model using train data which is relevent to the problem solution\n",
        "7. validate or evaulate the model performace\n",
        "8. test the data on unseen data (test data)\n",
        "9. check accuracy and maximize the model accuracy using model tuning using hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrcJfQwZAVuD"
      },
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58k5RdNZAVwl"
      },
      "source": [
        "-> because EDA stands for exploratory data analysis using it we wil understanding the distribution of dataset and accroding to that we make a decision for better model building and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PRFVa0nAVzV"
      },
      "source": [
        "12. What is correlation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZwwq_dzAV13"
      },
      "source": [
        "-> Correlation is a statistical measure that shows the strength and direction of the relationship between two variables.\n",
        "\n",
        "Value ranges from -1 to +1:\n",
        "\n",
        "+1 → perfect positive relationship\n",
        "\n",
        "-1 → perfect negative relationship\n",
        "\n",
        "0 → no linear relationship\n",
        "\n",
        "Example: As hours studied incraeses, marks scored increases → positive correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsM_B4sbAV4y"
      },
      "source": [
        "13. What does negative correlation mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOUhTkf-AV7w"
      },
      "source": [
        "-> Negative correlation means the direction of relation between two features are negative or opposite relation between each other.\n",
        "\n",
        "one feature is growing continuosly and other feature is down continuosly that's why it is negatively correlated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG6G5UJOAV-L"
      },
      "source": [
        "14. How can you find correlation between variables in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2cKZI7AWAl"
      },
      "source": [
        "-> for finding correlation we use corr() function\n",
        "\n",
        "for example finding the realtionship between two variables is....\n",
        "\n",
        "df[\"feature_1\"].corr(df[\"feature2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_JseCpHAWDN"
      },
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVObzC8VhytT"
      },
      "source": [
        "-> correlation is find how the two different features are correlated to each other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daS-9yRzAWFy"
      },
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry7KytZdAWIS"
      },
      "source": [
        "17. What is sklearn.linear_model ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7mbCoyTAWLw"
      },
      "source": [
        "-> sklearn.linear_model is a Scikit-learn module used for training models like Linear Regression and Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgTTY5wui_tR"
      },
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaR_HcRMjCrv"
      },
      "source": [
        "-> model.fit() means we train the model using train dataset.\n",
        "\n",
        "It learns the relationship between features (X) and target (y) by adjusting parameters\n",
        "\n",
        "Arguments to give in fit()\n",
        "\n",
        "X → Features/input data.\n",
        "\n",
        "y → Target/output labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXDaKVEjCuO"
      },
      "source": [
        "19. What does model.predict() do? What arguments must be given?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92EMe1yNjCxE"
      },
      "source": [
        "model.predict() is used to generate predictions on unseen/test data using the trained model.\n",
        "\n",
        "It takes only the input features as argument.\n",
        "\n",
        "y_pred = model.predict(X_test)   # only X_test is passed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFsIXSsJjCy7"
      },
      "source": [
        "20. What are continuous and categorical variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6fzTwj1jC04"
      },
      "source": [
        "-> Continuos varibale contains the numerical values like heigh, weight, etc.\n",
        "\n",
        "categorical variable contains the text data like male or female, yes or no, high or low, etc. this is called as categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2RgBIk_jC3T"
      },
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stVNEwjDjC50"
      },
      "source": [
        "-> Feature scaling is the process of normalizing or standardizing independent variables (features) so that they are on a similar scale.\n",
        "\n",
        "Example: One feature in kilometers (0–1000) and another in age (0–100) → without scaling, the model gives more importance to larger-valued features.\n",
        "\n",
        "It helps....\n",
        "\n",
        "Improves model performance\n",
        "\n",
        "Better accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YKgaHenjDAR"
      },
      "source": [
        "22. How do we perform scaling in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32mTL7XlmFKM"
      },
      "source": [
        "-> scaling is used to manage the faeature's data range in uniform or in fixed range (0-1)\n",
        "\n",
        "#Standradization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scale = StandardScaler()\n",
        "x_scaled = standard_scale.fit_transform(x)\n",
        "\n",
        "#Normlization\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#Unit Vector Scaling\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o6tNw98mFMm"
      },
      "source": [
        "23. What is sklearn.preprocessing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mdQWF5_mFPP"
      },
      "source": [
        "-> preprocessing is a module of sciket learn package. it is used for processing the data such that data scaling using minmaxscaler, standardscaler, normlize (unit scaling) and encoding using onehotencoder, labelencoding, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5E4-MFSmFR1"
      },
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-SC8yzZpqax"
      },
      "source": [
        "-> for industry level we mainly spilit the train data and test data in this ration 70-30 or 80-20 or 90-10 accrodingly.\n",
        "\n",
        "but commonly we use 70-30 ratio because it's good approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C35AtW7MpqdS"
      },
      "source": [
        "25. Explain data encoding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEPaISxppqgE"
      },
      "source": [
        "-> Data Encoding is the process of converting categorical (non-numeric) data into numeric format so that machine learning models can process it.\n",
        "\n",
        "Why is it needed?\n",
        "\n",
        "Models can't directly understand text.\n",
        "\n",
        "Encoding makes categorical data machine-readable.\n",
        "\n",
        "useful features will not be wasted\n",
        "\n",
        "#Types of Data Encoding\n",
        "\n",
        "-> Label Encoding\n",
        "\n",
        "Assigns an integer to each category.\n",
        "\n",
        "Example: Red=0, Green=1, Blue=2.\n",
        "\n",
        "-> One-Hot Encoding\n",
        "\n",
        "Creates separate binary columns for each category.\n",
        "\n",
        "Example: Color → [Red, Green, Blue] → 3 columns with 0/1.\n",
        "\n",
        "-> Ordinal Encoding\n",
        "\n",
        "Encodes categories with meaningful order.\n",
        "\n",
        "Example: Small=0, Medium=1, Large=2.\n",
        "\n",
        "-> Target guided ordinal encoding\n",
        "\n",
        "Replaces category with the mean of target variable.\n",
        "\n",
        "Example: Replace \"City\" with average house price for that city."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
